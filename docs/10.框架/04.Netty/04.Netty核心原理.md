#  Netty核心原理

## 1 Netty介绍

### 1.1 原生NIO存在的问题

1. NIO 的类库和 API 繁杂，使用麻烦：需要熟练掌握 Selector、ServerSocketChannel、SocketChannel、ByteBuffer等。
2. 需要具备其他的额外技能：要熟悉 Java 多线程编程，因为 NIO 编程涉及到 Reactor 模式，你必须对多线程和网络编程非常熟悉，才能编写出高质量的 NIO 程序。
3. 开发工作量和难度都非常大：例如客户端面临断连重连、网络闪断、半包读写、失败缓存、网络拥塞和异常流的处理等等。
4. JDK NIO 的 Bug：臭名昭著的 Epoll Bug，它会导致 Selector 空轮询，最终导致 CPU 100%。直到JDK 1.7版本该问题仍旧存在，没有被根本解决
在NIO中通过Selector的轮询当前是否有IO事件，根据JDK NIO api描述，Selector的select方法会一直阻塞，直到IO事件达到或超时，但是在Linux平台上这里有时会出现问题，在某些场景下select方法会直接返回，即使没有超时并且也没有IO事件到达，这就是著名的epoll bug，这是一个比较严重的bug，它会导致线程陷入死循环，会让CPU飙到100%，极大地影响系统的可靠性，到目前为止，JDK都没有完全解决这个问题。  

### 1.2 Netty概述

Netty 是由 JBOSS 提供的一个 Java 开源框架。Netty 提供异步的、基于事件驱动的网络应用程序框架，用以快速开发高性能、高可靠性的网络 IO 程序。 Netty 是一个基于 NIO 的网络编程框架，使用Netty 可以帮助你快速、简单的开发出一 个网络应用，相当于简化和流程化了 NIO 的开发过程。 作为当前最流行的 NIO 框架，Netty 在互联网领域、大数据分布式计算领域、游戏行业、 通信行业等获得了广泛的应用，知名的 Elasticsearch 、Dubbo 框架内部都采用了 Netty。  

![netty-1](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-1.png)

从图中就能看出 Netty 的强大之处：零拷贝、可拓展事件模型；支持 TCP、UDP、HTTP、
WebSocket 等协议；提供安全传输、压缩、大文件传输、编解码支持等等。
具备如下优点:
1. 设计优雅，提供阻塞和非阻塞的 Socket；提供灵活可拓展的事件模型；提供高度可定制的线程模型。
2. 具备更高的性能和更大的吞吐量，使用零拷贝技术最小化不必要的内存复制，减少资源的消耗，通过与其他业界主流的 NIO 框架对比，Netty 的综合性能优。 
3. 提供安全传输特性。
4. 支持多种主流协议；预置多种编解码功能，支持用户开发私有协议。  

## 2 线程模型

### 2.1 线程模型介绍

不同的线程模式，对程序的性能有很大影响，在学习Netty线程模式之前，首先讲解下 各个线程模式， 最后看看 Netty 线程模型有什么优越性.

目前存在的线程模型有：

- 传统阻塞 I/O 服务模型

- Reactor 模型

  根据 Reactor 的数量和处理资源池线程的数量不同，有 3 种典型的实现

  - 单 Reactor 单线程

  - 单 Reactor 多线程

  - 主从 Reactor 多线程

### 2.2 传统阻塞I/O服务模型

采用阻塞 IO 模式获取输入的数据, 每个连接都需要独立的线程完成数据的输入 , 业务处理和数据返回工作

![netty-2](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-2.png)

**存在问题**:

1. 当并发数很大，就会创建大量的线程，占用很大系统资源

2. 连接创建后，如果当前线程暂时没有数据可读，该线程会阻塞在 read 操作，造成线程资源浪费

### 2.3 Reactor模型

Reactor模型的核心是：Reactor+Handles。Reactor在一个单独的线程中运行，负责监听和分发事件，将接收到的io事件交给不同的Handle来处理响应。Handles是处理程序执行I/O事件的实际操作，Reactor通过调度适当的Handles来处理io事件。
**目前常用的Netty、Redis、Memcached、Nignx都是基于Reactor模式实现的。**

#### 2.3.1  **单** **Reactor** **单线程**

![netty-4](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-4.png)

**流程：**

1. Reactor 对象通过 **Select 监控客户端请求事件，收到事件后通过 Dispatch 进行分发。**
2. 如果是建立连接请求事件，则由 Acceptor 通过 Accept 处理连接请求，然后创建一个 Handler 对象处理连接完成后的后续业务处理。
3. 如果不是建立连接事件，则 Reactor 会分发调用连接对应的 Handler 来响应
4. Handler 会完成 Read→业务处理→Send 的完整业务流程。

**优点：**

模型简单，没有多线程、进程通信、竞争的问题，全部都在一个线程中完成。

**缺点：**

1. 性能问题，只有一个线程，无法完全发挥多核 CPU 的性能。Handler 在处理某个连接上的业务时，整个进程无法处理其他连接事件，很容易导致性能阻塞。
2. 可靠性问题，线程意外终止，或者进入死循环，会导致整个系统通信模块不可用，不能接收和处理外部消息，造成节点故障。

**使用场景：**
   客户端的数量有限，业务处理非常快速。java原生nio就是这个模型。

#### 2.3.2 单Reactor 多线程

![netty-5](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-5.png)



**流程：**

1. Reactor 对象**通过select 监控客户端请求事件, 收到事件后，通过dispatch进行分发。**

2. 如果建立连接请求, 则由Acceptor 通过accept 处理连接请求, 然后创建一个Handler对象处理完成连接后的各种事件。

3. 如果不是连接请求，则由reactor分发调用连接对应的handler 来处理。

4. handler 只负责响应事件，不做具体的业务处理, 通过read 读取数据后，会分发给后面的worker线程池的某个线程处理业务。

5. worker 线程池会分配独立线程完成真正的业务，并将结果返回给handler。

6. handler收到响应后，通过send 将结果返回给client。


**优点：**
   可以充分的利用多核cpu 的处理能力。

**缺点：**
   多线程数据共享和访问比较复杂， reactor 处理所有的事件的监听和响应，在单线程运行， 在高并发场景容易出现性能瓶颈.。

#### 2.3.3 主从Reactor多线程

![netty-6](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-6.png)

**流程：**

1. Reactor主线程 MainReactor 对象**通过select 监听连接事件, 收到事件后，通过Acceptor 处理连接事件。**
2. 当 Acceptor 处理连接事件后，MainReactor 将连接分配给SubReactor 。
3. subreactor 将连接加入到连接队列进行监听,并创建handler进行各种事件处理。
4. 当有新事件发生时， subreactor 就会调用对应的handler处理。
5. handler 通过read 读取数据，分发给后面的worker 线程处理。
6. worker 线程池分配独立的worker 线程进行业务处理，并返回结果。
7. handler 收到响应的结果后，再通过send 将结果返回给client。
8. Reactor 主线程可以对应多个Reactor 子线程, 即MainRecator 可以关联多个SubReactor。

**优点：**

1. 父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续的业务处理。
2. 父线程与子线程的数据交互简单，Reactor 主线程只需要把新连接传给子线程，子线程无需返回数据。

**缺点：**

这种模式的缺点是编程复杂度较高。但是由于其优点明显，在许多项目中被广泛使用，包括

Nginx、Memcached、Netty 等。这种模式也被叫做服务器的 1+M+N 线程模式，即使用该模式开

发的服务器包含一个（或多个，1 只是表示相对较少）连接建立线程+M 个 IO 线程+N 个业务处理

线程。这是业界成熟的服务器程序设计模式。

### 2.4 Netty线程模型

Netty 的设计主要基于主从 Reactor 多线程模式，并做了一定的改进。

**简单版Netty模型**

![netty-7](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-7.png)

- BossGroup 线程维护 Selector，ServerSocketChannel 注册到这个 Selector 上，只关注连接建立请求事件（主 Reactor）

- 当接收到来自客户端的连接建立请求事件的时候，通过 ServerSocketChannel.accept 方法获得对应的 SocketChannel，并封装成 NioSocketChannel 注册到 WorkerGroup 线程中的


- Selector，每个 Selector 运行在一个线程中（从 Reactor） 当 WorkerGroup 线程中的 Selector 监听到自己感兴趣的 IO 事件后，就调用 Handler 进行理


**进阶版Netty模型**

![netty-8](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-8.png)

- 有两组线程池：BossGroup 和 WorkerGroup，BossGroup 中的线程专门负责和客户端建立

  连接，WorkerGroup 中的线程专门负责处理连接上的读写

- BossGroup 和 WorkerGroup 含有多个不断循环的执行事件处理的线程，每个线程都包含一

  个 Selector，用于监听注册在其上的 Channel

- 每个 BossGroup 中的线程循环执行以下三个步骤

  - 轮训注册在其上的 ServerSocketChannel 的 accept 事件（OP_ACCEPT 事件）

  - 处理 accept 事件，与客户端建立连接，生成一个 NioSocketChannel，并将其注册到

    WorkerGroup 中某个线程上的 Selector 上
  - 再去以此循环处理任务队列中的下一个事件
    

- 每个 WorkerGroup 中的线程循环执行以下三个步骤
  
  - 轮训注册在其上的 NioSocketChannel 的 read/write 事件（OP_READ/OP_WRITE 事件）
  
  - 在对应的 NioSocketChannel 上处理 read/write 事件
  
  - 再去以此循环处理任务队列中的下一个事件


**详细版Netty模型**

![netty-9](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-9.png)

**模型解释：**

1) Netty 抽象出两组线程池BossGroup和WorkerGroup，BossGroup专门负责接收客户端的连接, WorkerGroup专门负责网络的读写

2) BossGroup和WorkerGroup类型都是NioEventLoopGroup

3) NioEventLoopGroup 相当于一个事件循环**线程组**, 这个组中含有多个事件循环线程 ， 每一个事件循环线程是NioEventLoop

4) 每个NioEventLoop都有一个selector , 用于监听注册在其上的socketChannel的网络通讯

5) 每个Boss  NioEventLoop线程内部循环执行的步骤有 3 步

- 处理accept事件 , 与client 建立连接 , 生成 NioSocketChannel 
- 将NioSocketChannel注册到某个worker  NIOEventLoop上的selector
- 处理任务队列的任务 ， 即runAllTasks

6) 每个worker  NIOEventLoop线程循环执行的步骤

- 轮询注册到自己selector上的所有NioSocketChannel 的read, write事件
- 处理 I/O 事件， 即read , write 事件， 在对应NioSocketChannel 处理业务
- runAllTasks处理任务队列TaskQueue的任务 ，一些耗时的业务处理一般可以放入TaskQueue中慢慢处理，这样不影响数据在 pipeline 中的流动处理

7) 每个worker NIOEventLoop处理NioSocketChannel业务时，会使用 pipeline (管道)，管道中维护了很多 handler 处理器用来处理 channel 中的数据

## 3 **Netty模块组件**

### 3.1 **ServerBootstrap和Bootstrap**

Bootstrap 意思是引导，一个 Netty 应用通常由一个 Bootstrap 开始，主要作用是配置整个 Netty 程序，串联各个组件，Netty 中 Bootstrap 类是客户端程序的启动引导类，ServerBootstrap 是服务端启动引导类。

- public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup)， 该方法用于服务器端，用来设置两个 EventLoop
- public B group(EventLoopGroup group) ，该方法用于客户端，用来设置一个 EventLoop
- public B channel(Class<? extends C> channelClass)，该方法用来设置一个服务器端的通道 实现
- public B option(ChannelOption option, T value)，用来给 ServerChannel 添加配置
- public ServerBootstrap childOption(ChannelOption childOption, T value)，用来给接收到的通道添加配置
- public ServerBootstrap childHandler(ChannelHandler childHandler)，该方法用来设置业务 处理类（自定义的 handler)
- public ChannelFuture bind(int inetPort) ，该方法用于服务器端，用来设置占用的端口号
- public ChannelFuture connect(String inetHost, int inetPort) ，该方法用于客户端，用来连 接服务器端

### 3.2 **Future**和ChannelFuture

在 Netty 中所有的 IO 操作都是异步的，不能立刻得知消息是否被正确处理。

但是可以过一会等它执行完成或者直接注册一个监听，具体的实现就是通过 Future 和 ChannelFutures，他们可以注册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件。

常用方法如下所示：

- Channel channel()，返回当前正在进行 IO 操作的通道

- ChannelFuture sync()，等待异步操作执行完毕,将异步改为同步

### 3.3 **Channel**

Netty 网络通信的组件，能够用于执行网络 I/O 操作。

Channel 为用户提供：

1. 当前网络连接的通道的状态（例如是否打开？是否已连接？）
2. 网络连接的配置参数 （例如接收缓冲区大小）
3. 提供异步的网络 I/O 操作(如建立连接，读写，绑定端口)，异步调用意味着任何 I/O 调用都将立即返回，并且不保证在调用结束时所请求的 I/O 操作已完成。
4. 调用立即返回一个 ChannelFuture 实例，通过注册监听器到 ChannelFuture 上，可以 I/O 操作成功、失败或取消时回调通知调用方。
5. 支持关联 I/O 操作与对应的处理程序。

不同协议、不同的阻塞类型的连接都有不同的 Channel 类型与之对应。

下面是一些常用的 Channel 类型：

```java
NioSocketChannel，异步的客户端 TCP Socket 连接。 
NioServerSocketChannel，异步的服务器端 TCP Socket 连接。 
NioDatagramChannel，异步的 UDP 连接。 
NioSctpChannel，异步的客户端 Sctp 连接。 
NioSctpServerChannel，异步的 Sctp 服务器端连接。 这些通道涵盖了 UDP 和 TCP 网络 IO 以及文件 IO。
```

### 3.4 **Selector**

Netty 基于 Selector 对象实现 I/O 多路复用，通过 Selector 一个线程可以监听多个连接的 Channel 事件。

当向一个 Selector 中注册 Channel 后，Selector 内部的机制就可以自动不断地查询(Select) 这些注册的 Channel 是否有已就绪的 I/O 事件（例如可读，可写，网络连接完成等），这样程序就可以很简单地使用一个线程高效地管理多个 Channel 。

### 3.5  **NioEventLoop**

NioEventLoop 中维护了一个线程和任务队列，支持异步提交执行任务，线程启动时会调用 NioEventLoop 的 run 方法，执行 I/O 任务和非 I/O 任务：

- I/O 任务，即 selectionKey 中 ready 的事件，如 accept、connect、read、write 等，由 processSelectedKeys 方法触发。

- 非 IO 任务，添加到 taskQueue 中的任务，如 register0、bind0 等任务，由 runAllTasks 方法触发。 

每个 EventLoop 维护着一个 Selector 实例。

### 3.6 NioEventLoopGroup

NioEventLoopGroup，主要管理 eventLoop 的生命周期，可以理解为一个线程池，内部维护了一组线程，每个线程(NioEventLoop)负责处理多个 Channel 上的事件，而一个 Channel 只对应于一个线程。

### 3.7 **ChannelHandler**

ChannelHandler 是一个接口，处理 I/O 事件或拦截 I/O 操作，并将其转发到其 ChannelPipeline(业务处理链)中的下一个处理程序。

ChannelHandler 本身并没有提供很多方法，因为这个接口有许多的方法需要实现，方便使用期间，可以继承它的子类：

```shell
ChannelInboundHandler 用于处理入站 I/O 事件。
ChannelOutboundHandler 用于处理出站 I/O 操作。

ChannelInboundHandlerAdapter 用于处理入站 I/O 事件。
ChannelOutboundHandlerAdapter 用于处理出站 I/O 操作。
```

![netty-10](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-10.png)

Netty开发中需要自定义一个 Handler 类去实现 ChannelHandle接口或其子接口或其实现类，然后

通过重写相应方法实现业务逻辑，我们接下来看看一般都需要重写哪些方法

- public void channelActive(ChannelHandlerContext ctx)，通道就绪事件

- public void channelRead(ChannelHandlerContext ctx, Object msg)，通道读取数据事件

- public void channelReadComplete(ChannelHandlerContext ctx) ，数据读取完毕事件

- public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause)，通道发生异常事件

### 3.8 **ChannelPipeline**

ChannelPipeline 是一个 Handler 的集合，它负责处理和拦截 inbound 或者 outbound 的事件和操作，相当于一个贯穿 Netty 的责任链。

ChannelPipeline 实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，以及 Channel 中各个的 ChannelHandler 如何相互交互。

在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应，它们的组成关系如下：

![netty-11](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-11.png)

一个 Channel 包含了一个 ChannelPipeline，而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表，并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler。

read事件(入站事件)和write事件(出站事件)在一个双向链表中，入站事件会从链表 head 往后传递到最后一个入站的 handler，出站事件会从链表 tail 往前传递到最前一个出站的 handler，两种类型的 handler 互不干扰。

**以客户端应用程序为例**，如果事件的运动方向是从客户端到服务端的，那么我们称这些事件为**出站的**，即客户端发送给服务端的数据会通过pipeline中的一系列**ChannelOutboundHandler(ChannelOutboundHandler调用是从tail到head方向逐个调用每个handler的逻辑)**，并被这些Handler处理，反之则称为**入站的，**入站只调用pipeline里的**ChannelInboundHandler**逻辑**(ChannelInboundHandler调用是从head到tail方向**逐个调用每个handler的逻辑)。

![netty-12](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-12.png)

### 3.9 **ChannelHandlerContext**

保存 Channel 相关的所有上下文信息，同时关联一个 ChannelHandler 对象。

这 是 事 件 处 理 器 上 下 文 对 象 ， Pipeline 链 中 的 实 际 处 理 节 点 。 每 个 处 理 节 点

ChannelHandlerContext 中 包 含 一 个 具 体 的 事 件 处 理 器 ChannelHandler ,同时

ChannelHandlerContext 中也绑定了对应的 ChannelPipeline和 Channel 的信息，方便对

ChannelHandler 进行调用。常用方法如下所示：

- ChannelFuture close()，关闭通道

- ChannelOutboundInvoker flush()，刷新

- ChannelFuture writeAndFlush(Object msg) ， 将 数 据 写 到 ChannelPipeline 中 当 前

- ChannelHandler 的下一个 ChannelHandler 开始处理（出站）

### 3.10 **ChannelOption**

Netty 在创建 Channel 实例后,一般都需要设置 ChannelOption 参数。ChannelOption 是 Socket 的标

准参数，而非 Netty 独创的。常用的参数配置有：

- ChannelOption.SO_BACKLOG

  对应 TCP/IP 协议 listen 函数中的 backlog 参数，用来初始化服务器可连接队列大小。服务端处理

  客户端连接请求是顺序处理的，所以同一时间只能处理一个客户端连接。多个客户 端来的时候，服

  务端将不能处理的客户端连接请求放在队列中等待处理，backlog 参数指定 了队列的大小。

- ChannelOption.SO_KEEPALIVE ，一直保持连接活动状态。该参数用于设置TCP连接，当设置该选

  项以后，连接会测试链接的状态，这个选项用于可能长时间没有数据交流的连接。当设置该选项以

  后，如果在两小时内没有数据的通信时，TCP会自动发送一个活动探测数据报文。

##  4 **Netty异步模型**

异步的概念和同步相对。当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的组件在完成后，通过状态、通知和回调来通知调用者。

![netty-13](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-13.png)

Netty 中的 I/O 操作是异步的，包括 Bind、Write、Connect 等操作会简单的返回一个ChannelFuture。

调用者并不能立刻获得结果，而是通过 Future-Listener 机制，用户可以方便的主动获

取或者通过通知机制获得IO 操作结果. 

Netty 的异步模型是建立在 future 和 callback 的之上的。

callback 就是回调。重点说 Future，它的核心思想是：假设一个方法 fun，计算过程可能非常耗时，等

待 fun 返回显然不合适。那么可以在调用 fun 的时候，立马返回一个 Future，后续可以通过 Future 去

监控方法 fun 的处理过程(即 ： Future-Listener 机制)

### 4.1 Future

表示异步的执行结果, 可以通过它提供的方法来检测执行是否完成，ChannelFuture 是他的一

个子接口. ChannelFuture 是一个接口 ,可以添加监听器，当监听的事件发生时，就会通知到监听器

当 Future 对象刚刚创建时，处于非完成状态，调用者可以通过返回的 ChannelFuture 来获取

操作执行的状态， 注册监听函数来执行完成后的操作。

**常用方法有:**

- sync 方法, 阻塞等待程序结果反回

- isDone 方法来判断当前操作是否完成；

- isSuccess 方法来判断已完成的当前操作是否成功；

- getCause 方法来获取已完成的当前操作失败的原因；

- isCancelled 方法来判断已完成的当前操作是否被取消；

- addListener 方法来注册监听器，当操作已完成(isDone 方法返回完成)，将会通知指定的监听

  器；如果Future 对象已完成，则通知指定的监听器

### 4.2 Future-Listener 机制

给Future添加监听器,监听操作结果

代码实现:

```java
ChannelFuture channelFuture = ctx.writeAndFlush(buf);
channelFuture.addListener(new ChannelFutureListener() {
    @Override
    public void operationComplete(ChannelFuture future) throws Exception {
        if (future.isSuccess()) {
            System.out.println("数据发送成功.");
        } else {
            System.out.println("数据发送失败.");
        }
    }
});
```

## 5  Netty编解码

### 5.1 编码和解码的基本介绍

编写网络应用程序时，因为数据在网络中传输的都是二进制字节码数据，在发送数据时就需要编码，接收数据时就需要解码。

codec（编解码器）的组成部分有两个：decoder（解码器）和 encoder（编码器）。encoder 负责把业务数据转换成字节码数据，decoder 负责把字节码数据转换成业务数据。

![netty-14](https://gitee.com/linbingxing/image/raw/master/java/netty/netty-14.png)

为啥jdk有编解码，还要netty自己开发编解码？

java自带序列化的缺点

1）无法跨语言
2) 序列化后的码流太大，也就是数据包太大
3) 序列化和反序列化性能比较差

Netty里面的编解码：

- 解码器：负责处理“入站 InboundHandler”数据
- 编码器：负责“出站 OutboundHandler” 数据
- Netty里面提供默认的编解码器，也支持自定义编解码器
  - Encoder:编码器
  - Decoder:解码器
  - Codec:编解码器

## 6 Netty粘包和拆包

### 6.1  **粘包和拆包简介**

粘包和拆包是TCP网络编程中不可避免的，无论是服务端还是客户端，当我们读取或者发送消息的时候，都需要考虑TCP底层的粘包/拆包机制。

TCP是个“流”协议，所谓流，就是没有界限的一串数据。TCP底层并不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行包的划分，所以在业务上认为，一个完整的包可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。