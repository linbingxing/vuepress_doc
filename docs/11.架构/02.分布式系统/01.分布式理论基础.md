#  分布式理论基础

#  **分布式理论与分布式架构设计理论**

## **1.** **分布式架构介绍**

###  **1.1** **什么是分布式系统**

在《分布式系统概念与设计》一书中，对分布式系统做了如下定义：

> 分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统

简单来说就是一群独立计算机集合共同对外提供服务，但是对于系统的用户来说，就像是一台计算机在提供服务一样。分布式意味着可以采用更多的普通计算机（相对于昂贵的大型机）组成分布式集群对外提供服务。计算机越多，CPU、内存、存储资源等也就越多，能够处理的并发访问量也就越大。

通俗来说，所谓分布式系统，就是一个业务拆分成多个子业务，分布在不同的服务器节点，共同构成的系统称为分布式系统，同一个分布式系统中的服务器节点在空间部署上是可以随意分布的，这些服务器可能放在不同的机柜 中，也可能在不同的机房中，甚至分布在不同的城市。

### **1.2** **分布式与集群的区别**

集群: 多个服务器做同一个事情

分布式: 多个服务器做不同的事情

###  **1.3** **分布式系统特性**

1. 分布性

   空间中随机分布。这些计算机可以分布在不同的机房，不同的城市，甚至不同的国家。

2. 对等性

   分布式系统中的计算机没有主/从之分，组成分布式系统的所有节点都是对等的。

3. 并发性

   同一个分布式系统的多个节点，可能会并发地操作一些共享的资源，诸如数据库或分布式存储。

4. 缺乏全局时钟

   既然各个计算机之间是依赖于交换信息来进行相互通信，很难定义两件事件的先后顺序，缺乏全局

始终控制序列。

5. 故障总会发生

   组成分布式的计算机，都有可能在某一时刻突然间崩掉。分的计算机越多，可能崩掉一个的几率就

越大。如果再考虑到设计程序时的异常故障，也会加大故障的概率。

6. 处理单点故障

   单点SPoF（Single Point of Failure）：某个角色或者功能只有某一台计算机在支撑，在这台计算

机上出现的故障是单点故障。

###  **1.4** **分布式系统面临的问题**

1. 通信异常

   网络本身的不可靠性，因此每次网络通信都会伴随着网络不可用的风险（光纤、路由、DNS等硬件

设备或系统的不可用），都会导致最终**分布式系统无法顺利进行一次网络通信**，另外，即使分布式

系统各节点之间的网络通信能够正常执行，其延时也会大于单机操作，存在巨大的延时差别，也会

影响消息的收发过程，因此消息丢失和消息延迟变的非常普遍。

2. 网络分区

   网络之间出现了网络不连通，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被

切分成了若干个孤立的区域，**分布式系统就会出现局部小集群**，在极端情况下，这些小集群会独立

完成原本需要整个分布式系统才能完成的功能，包括数据的事务处理，这就对分布式一致性提出非

常大的挑战。

3. 节点故障

   节点故障是分布式系统下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机

或"僵死"现象，根据经验来说，每个节点都有可能出现故障，并且经常发生。

4. 三态

   分布式系统每一次请求与响应存在特有的“三态”概念，即成功、失败和超时。

5. 重发

   分布式系统在发生调用的时候可能会出现 失败 超时 的情况. 这个时候需要重新发起调用。

6. 幂等

   一次和多次请求某一个资源对于资源本身应该具有同样的结果（网络超时等问题除外）。也就是

   说，其任意多次执行对资源本身所产生的影响均与一次执行的影响相同。

##  2.分布式理论

### **2.1** **数据一致性**

在分布式系统中要解决的一个重要问题就是数据的复制。

分布式系统对于数据的复制需求一般都来自于以下两个原因：

1、为了增加系统的可用性，以防止单点故障引起的系统不可用。

2、提高系统的整体性能，通过负载均衡技术，能够让分布在不同地方的数据副本都能够为用户提供服务。

数据复制在可用性和性能方面给分布式系统带来的巨大好处是不言而喻的，然而数据复制所带来的一致性挑战，也是每一个系统研发人员不得不面对的。

所谓分布一致性问题，是指在分布式环境中引入数据复制机制之后，不同数据节点之间可能出现的，并无法依靠计算机应用程序自身解决的数据不一致的情况。简单讲，数据一致性就是指在对一个副本数据进行更新的时候，必须确保也能够更新其他的副本，否则不同副本之间的数据将不一致。

如何能既保证数据一致性，又保证系统的性能，是每一个分布式系统都需要重点考虑和权衡的。

#### **2.1.1** **什么是分布式数据一致性**

分布式数据一致性，指的是数据在多份副本中存储时，各副本中的数据是一致的。

![数据一致性](https://gitee.com/linbingxing/image/raw/master/distributed/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7.png)

#### **2.1.2** **副本一致性**

分布式系统当中，数据往往会有多个**副本**。多个副本就需要保证数据的一致性。这就带来了同步的

问题，因为网络延迟等因素, 我们几乎没有办法保证可以同时更新所有机器当中的包括备份所有数据. 就

会有数据不一致的情况。

![副本一致性](https://gitee.com/linbingxing/image/raw/master/distributed/%E5%89%AF%E6%9C%AC%E4%B8%80%E8%87%B4%E6%80%A7.png)

总得来说，我们无法找到一种能够满足分布式系统中数据一致性解决方案。因此，如何既保证数据的一

致性，同时又不影响系统运行的性能，是每一个分布式系统都需要重点考虑和权衡的。于是，一致性级

别由此诞生。

####   **2.1.3** **一致性分类**

一般来说，数据一致性模型可以分为强一致性和弱一致性，强一致性也叫做线性一致性，除此以外，所有其他的一致性都是弱一致性的特殊情况。弱一致性根据不同的业务场景，又可以分解为更细分的模型，不同一致性模型又有不同的应用场景。

在互联网领域的绝大多数场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。

![一致性模型](https://gitee.com/linbingxing/image/raw/master/%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B.png)

1. 强一致性

   所谓的强一致性（Strict Consistency），也称为原子一致性（Atomic Consistency）或者线性(Linearizability)。 它对一致性的要求两个：

- 任何一次读都能读到某个数据的最近一次写的数据。

- 系统中的所有进程，看到的操作顺序，都和全局时钟下的顺序一致。

  当更新操作完成之后，任何多个后续进程的访问都会返回最新的更新过的值，这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。根据 CAP 理论，这种实现需要牺牲可用性。

2. 弱一致性

   系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。用户读到某一操作对系统数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。

3. 最终一致性
   最终一致性是弱一致性的特例，强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

#### **2.1.4 **最终一致性模型变种

- 因果一致性（Causal consistency）

如果节点A在更新完某个数据后通知了节点B,那么节点B的访问修改操作都是基于A更新后的值,同时,和节点A没有因果关系的C的数据访问则没有这样的限制。

因果一致性的应用场景可以举个例子，在微博或者微信进行评论的时候，比如你在朋友圈发了一张照片，朋友给你评论了，而你对朋友的评论进行了回复，这条朋友圈的显示中，你的回复必须在朋友之后，这是一个因果关系，而其他没有因果关系的数据，可以允许不一致。

![因果关系一致性](https://gitee.com/linbingxing/image/raw/master/distributed/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E4%B8%80%E8%87%B4%E6%80%A7.png)

- 读己之所写（Read your writes）

因果一致性的特定形式，一个节点A总可以读到自己更新的数据。

![读已所写一致性](https://gitee.com/linbingxing/image/raw/master/distributed/%E8%AF%BB%E5%B7%B2%E6%89%80%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7.png)

- 会话一致性（Session consistency）

访问存储系统同一个有效的会话，系统应保证该进程读己之所写。

会话一致性将对系统数据的访问过程框定在了一个会话当中，约定了系统能保证在同一个有效的会话中实现“读己之所写”的一致性，就是在你的一次访问中，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。

![会话一致性](https://gitee.com/linbingxing/image/raw/master/distributed/%E4%BC%9A%E8%AF%9D%E4%B8%80%E8%87%B4%E6%80%A7.png)

- 单调读一致性（Monotonic read consistency）

一个节点从系统中读取一个特定值之后，那么该节点从系统中不会读取到该值以前的任何值。

![单调读一致性](https://gitee.com/linbingxing/image/raw/master/distributed/%E5%8D%95%E8%B0%83%E8%AF%BB%E4%B8%80%E8%87%B4%E6%80%A7.png)

- 单调写一致性（Monotonic write consistency）

一个系统要能够保证来自同一个节点的写操作被顺序执行（保证写操作串行化）。

![单调写一致性](https://gitee.com/linbingxing/image/raw/master/distributed/%E5%8D%95%E8%B0%83%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7.png)

### 2.2 CAP定理

####  2.2.1 CAP定理介绍

CAP 定理又被称作**布鲁尔定理**，是加州大学的计算机科学家布鲁尔在 2000 年提出的一个猜想；2002 年，麻省理工学院的赛斯·吉尔伯特和南希·林奇发表了布鲁尔猜想的证明，使之成为分布式计算领域公认的一个定理。CAP 定理认为，在分布式系统中，系统的一致性（Consistency）、可用性（Availability）、分区容忍性（Partition tolerance）三者不可能同时兼顾。

CAP 理论可以表述为，一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容忍性（Partition Tolerance）这三项中的两项。

1. 一致性（C-Consistency)

   **这里指的是强一致性**，是指“所有节点同时看到相同的数据”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，等同于所有节点拥有数据的最新版本。![CAP一致性](https://gitee.com/linbingxing/image/raw/master/distributed/CAP%E4%B8%80%E8%87%B4%E6%80%A7.png)

- 客户端向G1写入数据v1，并等待响应。
- 此时，G1服务器的数据为v1，而G2服务器的数据为v0，两者不一致。
- 接着，在返回响应给客户端之前，G2服务器会自动同步G1服务器的数据，使得G2服务器的数据也是v1。
- 一致性保证了不管向哪台服务器（比如这边向G1）写入数据，其他的服务器（G2）能实时同步数据。
- G2已经同步了G1的数据，会告诉G1，我已经同步了。
- G1接收了所有同步服务器的已同步的报告，才将“写入成功”信息响应给client。
- Client再发起请求，读取G2的数据。
- 此时得到的响应是v1，即使client读取数据到G2。

2. 可用性（A-Availability)

   系统中非故障节点收到的每个请求都必须有响应. 在可用系统中，如果我们的客户端向服务器发送请求，并且服务器未崩溃，则服务器必须最终响应客户端，不允许服务器忽略客户的请求。

3. 分区容错性（P-Partition tolerance）

具体是指“当部分节点出现消息丢失或者分区故障的时候，分布式系统仍然能够继续运行”，即系统容忍网络出现分区，并且在遇到某节点或网络分区之间网络不可达的情况下，仍然能够对外提供满足一致性和可用性的服务。允许网络丢失从一个节点发送到另一个节点的任意多条消息，即不同步. 也就是说，G1和G2发送给

对方的任何消息都是可以放弃的，也就是说G1和G2可能因为各种意外情况，导致无法成功进行同步，**分布式系统要能容忍这种情况。**

   ![分区容错性](https://gitee.com/linbingxing/image/raw/master/distributed/%E5%88%86%E5%8C%BA%E5%AE%B9%E9%94%99%E6%80%A7.png)

#### 2.2.2 CAP三者不可能同时满足论证

假设确实存在三者能同时满足的系统，

1. 那么我们要做的第一件事就是分区我们的系统，由于满足**分区容错性**，也就是说可能因为通信不佳

等情况，G1和G2之间是没有同步。

![验证CAP-1](https://gitee.com/linbingxing/image/raw/master/distributed/%E9%AA%8C%E8%AF%81CAP-1-1636809711959.png)

2. 接下来，我们的客户端将v1写入G1，但G1和G2之间是不同步的，所以如下G1是v1数据，G2是v0

数据。

![验证CAP-2](https://gitee.com/linbingxing/image/raw/master/distributed/%E9%AA%8C%E8%AF%81CAP-2.png)

3. 由于要满足可用性，即一定要返回数据，所以G1必须在数据没有同步给G2的前提下返回数据给

client，如下

![验证CAP-3](https://gitee.com/linbingxing/image/raw/master/distributed/%E9%AA%8C%E8%AF%81CAP-3.png)

4.  接下去，client请求的是G2服务器，由于G2服务器的数据是v0，所以client得到的数据是v0。

![验证CAP-4](https://gitee.com/linbingxing/image/raw/master/distributed/%E9%AA%8C%E8%AF%81CAP-4.png)

**结论**很明显，G1返回的是v1数据，G2返回的是v0数据，两者不一致。其余情况也有类似推导，也就是说 

CAP三者不能同时出现。

####  2.2.3 CAP 三者如何权衡

在分布式系统中，由于系统的各层拆分，涉及多节点间的通信和交互，节点间的分区故障是必然发生的，所以在分布式系统中分区容错性是必须要考虑的，因此P 是确定的，CAP 的应用模型就是 CP 架构和 AP 架构。分布式系统所关注的，就是在 Partition Tolerance 的前提下，如何实现更好的 A 和更稳定的 C。

- CP 架构：对于 CP 来说，放弃可用性，追求一致性和分区容错性。


我们熟悉的 ZooKeeper，就是采用了 CP 一致性，ZooKeeper 是一个分布式的服务框架，主要用来解决分布式集群中应用系统的协调和一致性问题。其核心算法是 Zab，所有设计都是为了一致性。在 CAP 模型中，ZooKeeper 是 CP，这意味着面对网络分区时，为了保持一致性，它是不可用的。关于 Zab 协议，将会在后面的 ZooKeeper 课时中介绍。

- AP 架构：对于 AP 来说，放弃强一致性，追求分区容错性和可用性，这是很多分布式系统设计时的选择，后面的 Base 也是根据 AP 来扩展的。

### 2.3 BASE理论

**BASE 模型**是 eBay 工程师提出大规模分布式系统的实践总结，其理念在于随着时间的迁移，不同节点的数据总是向同一个方向有一个相同的变化。在多数互联网应用情况下，其实我们也并非一定要求强一致性，部分业务可以容忍一定程度的延迟一致，所以为了兼顾效率，发展出来了最终一致性理论 BASE，BASE 是指基本可用（Basically Available）、软状态（Soft State）、最终一致性（Eventual Consistency）。

1. Basically Available(基本可用)

   基本可用（Basically Available）：基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。具体可以表现在流量削峰、延迟响应、体验降级、过载保护4个方面。

   常见如下情况

   - 响应时间上的损失：正常情况下的搜索引擎0.5秒即返回给用户结果，而基本可用看的搜索结果可能要1秒，2秒甚至3秒（超过3秒用户就接受不了了）
   - 功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单，但是到了促销时间，可能为了应对并发，保护购物系统的稳定性，部分用户会被引导到一个降级页面

   具体可以表现如下：

   - 电商网站在双十一大促等访问压力较大的时候，关闭商品排行榜等次要功能的展示，从而保证商品交易主流程的可用性，这也是我们常说的**服务降级**；


   - 为了错开双十一高峰期，电商网站会将预售商品的支付时间延后十到二十分钟，这就是**流量削峰**；


   - 在你抢购商品的时候，往往会在队列中等待处理，这也是常用的**延迟队列**。

2. Soft state（软状态）

   什么是软状态呢？相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种 “硬状态”。

   软状态指的是：允许系统中的数据存在中间状态，并认为该状态不会影响系统的整体可用性，即允

   许系统在多个不同节点的数据副本存在数据延时。

3. Eventually consistent（最终一致性）

   上面说软状态，然后不可能一直是软状态，必须有个时间期限。在期限过后，应当保证所有副本保

   持数据一致性。从而达到数据的最终一致性。这个时间期限取决于网络延时，系统负载，数据复制

   方案设计等等因素。

   在系统设计中，最终一致性实现的时间取决于网络延时、系统负载、不同的存储选型、不同数据复制方案设计等因素。

   如何实现最终一致性呢？你首先要知道它以什么为准，一般来说，在实际工程实践中有这样几种方式：

   - 以最新写入的数据为准，比如AP模型的KV存储采用就是这种方式
   - 以第一次写入的数据为准，如果不希望存储的数据被更改，以它为准。

   实现最终一致性的具体方式，常用的有以下几种。

   - 读时修复：在读取数据时，检测数据的不一致，进行修复。
   - 写时修复：在写入数据，检测数据的不一致时，进行修复。如果写失败就将数据缓存下来，然后定时重传，修复数据的不一致性。
   - 异步修复：通过定时对账检测副本数据的一致性，并修复。

##  3.分布式一致性协议

### 3.1 两阶段提交协议(2PC)

#### 3.1.1 两阶段提交协议

两阶段提交协议，简称2PC(2 Prepare Commit)，是比较常用的解决分布式事务问题的方式，要么所

有参与进程都提交事务，要么都取消事务，即实现ACID中的原子性(A)的常用手段。

二阶段提交算法的成立是基于以下假设的：

- 在该分布式系统中，存在一个节点作为**协调者**（Coordinator），其他节点作为**参与者**（Participants），且节点之间可以进行网络通信；


- 所有节点都采用预写式日志，日志被写入后被保存在可靠的存储设备上，即使节点损坏也不会导致日志数据的丢失；


- 所有节点不会永久性损坏，即使损坏后仍然可以恢复。


两阶段提交中的两个阶段，指的是 Commit-request 阶段和 Commit 阶段，两阶段提交的流程如下：

![2PC](https://gitee.com/linbingxing/image/raw/master/distributed/2PC.png)

#### 3.1.2 2PC执行流程

1. 成功执行事务事务提交流程

![2PC-1](https://gitee.com/linbingxing/image/raw/master/distributed/2PC-1.png)

**阶段一**:

- 事务询问

  协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者

  的响应。

- 执行事务 (写本地的Undo/Redo日志)

- 各参与者向协调者反馈事务询问的响应

**阶段二**:

- 发送提交请求：

  协调者向所有参与者发出 commit 请求。

- 事务提交：

  参与者收到 commit 请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执

  行期间占用的事务资源。

- 反馈事务提交结果：

  参与者在完成事务提交之后，向协调者发送 Ack 信息。

- 完成事务：

  协调者接收到所有参与者反馈的 Ack 信息后，完成事务。

2. 中断事务流程

   假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参

   与者的反馈响应，那么就会中断事务。

   ![2PC-2](https://gitee.com/linbingxing/image/raw/master/distributed/2PC-2.png)

   **阶段一**:

   - 事务询问

     协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者

     的响应。

   - 执行事务 (写本地的Undo/Redo日志)

   - 各参与者向协调者反馈事务询问的响应

   **阶段二**:

   - 发送回滚请求：

     协调者向所有参与者发出 Rollback 请求。

   - 事务回滚：

     参与者接收到 Rollback 请求后，会利用其在阶段一中记录的 Undo 信息来执行事务回滚操

     作，并在完成回滚之后释放在整个事务执行期间占用的资源。

   - 反馈事务回滚结果：

     参与者在完成事务回滚之后，向协调者发送 Ack 信息。

   - 中断事务：

     协调者接收到所有参与者反馈的 Ack 信息后，完成事务中断。

#### 3.1.3 2PC 存在的问题

![2PC-3](https://gitee.com/linbingxing/image/raw/master/distributed/2PC-3.png)

两阶段提交协议有几个明显的问题，下面列举如下。

- 资源被同步阻塞

  在执行过程中，所有参与节点都是事务独占状态，当参与者占有公共资源时，那么第三方节点访问公共资源会被阻塞。

- 协调者可能出现单点故障

  一旦协调者发生故障，参与者会一直阻塞下去。

- 在 Commit 阶段出现数据不一致

  在第二阶段中，假设协调者发出了事务 Commit 的通知，但是由于网络问题该通知仅被一部分参与者所收到并执行 Commit，其余的参与者没有收到通知，一直处于阻塞状态，那么，这段时间就产生了数据的不一致性。

### 3.2 三阶段提交协议(3PC)

为了解决二阶段协议中的同步阻塞等问题，三阶段提交协议在协调者和参与者中都引入了超时机制，并且把两阶段提交协议的第一个阶段拆分成了两步：询问，然后再锁资源，最后真正提交。

3PC，全称 “three phase commit”，是 2PC 的改进版，将 2PC 的 “提交事务请求” 过程一分为二，共形成了由CanCommit、PreCommit和doCommit三个阶段组成的事务处理协议。

![3PC](https://gitee.com/linbingxing/image/raw/master/distributed/3PC.png)

#### 3.2.1  三个阶段详解

1. **CanCommit 阶段**
   3PC 的 CanCommit 阶段其实和 2PC 的准备阶段很像。协调者向参与者发送 Can-Commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。

2. **PreCommit 阶段**
   协调者根据参与者的反应情况来决定是否可以继续事务的 PreCommit 操作。根据响应情况，有以下两种可能。

- 假如协调者从所有的参与者获得的反馈都是 Yes 响应，那么就会进行事务的预执行：

  发送预提交请求，协调者向参与者发送 PreCommit 请求，并进入 Prepared 阶段；

  事务预提交，参与者接收到 PreCommit 请求后，会执行事务操作；

  响应反馈，如果参与者成功执行了事务操作，则返回 ACK 响应，同时开始等待最终指令。

- 假如有任何一个参与者向协调者发送了 No 响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就中断事务：

  发送中断请求，协调者向所有参与者发送 abort 请求；

  中断事务，参与者收到来自协调者的 abort 请求之后，执行事务的中断。

3. **DoCommit 阶段**
   该阶段进行真正的事务提交，也可以分为以下两种情况。

- 执行提交

  发送提交请求。协调者接收到参与者发送的 ACK 响应后，那么它将从预提交状态进入到提交状态，并向所有参与者发送 doCommit 请求。

  事务提交。参与者接收到 doCommit 请求之后，执行正式的事务提交，并在完成事务提交之后释放所有事务资源。

  响应反馈。事务提交完之后，向协调者发送 ACK 响应。

  完成事务。协调者接收到所有参与者的 ACK 响应之后，完成事务。

- 中断事务
  协调者没有接收到参与者发送的 ACK 响应，可能是因为接受者发送的不是 ACK 响应，也有可能响应超时了，那么就会执行中断事务。

- 超时提交
  参与者如果没有收到协调者的通知，超时之后会执行 Commit 操作。

![3PC-1](https://gitee.com/linbingxing/image/raw/master/distributed/3PC-1.png)

#### 3.2.2 2PC对比3PC

1. 首先对于协调者和参与者都设置了超时机制（在2PC中，只有协调者拥有超时机制，即如果在一定

   时间内没有收到参与者的消息则默认失败）,主要是避免了参与者在长时间无法与协调者节点通讯

   （协调者挂掉了）的情况下，无法释放资源的问题，因为参与者自身拥有超时机制会在超时后，自

   动进行本地commit从而进行释放资源。而这种机制也侧面降低了整个事务的阻塞时间和范围。

2. 通过**CanCommit**、**PreCommit**、**DoCommit**三个阶段的设计，相较于2PC而言，多设置了一个**缓冲阶段**保证了在最后提交阶段之前各参与节点的状态是一致的 。 

3. PreCommit是一个缓冲，保证了在最后提交阶段之前各参与节点的状态是一致的。

####  3.2.3 2PC和3PC提交的应用

两阶段提交是一种比较精简的一致性算法/协议，很多关系型数据库都是采用两阶段提交协议来完成分布式事务处理的，典型的比如 MySQL 的 XA 规范。

在事务处理、数据库和计算机网络中，两阶段提交协议提供了分布式设计中的数据一致性的保障，整个事务的参与者要么一致性全部提交成功，要么全部回滚。MySQL Cluster 内部数据的同步就是用的 2PC 协议。

### 3.3 Gossip协议

#### 3.3.1 什么是Gossip协议

Gossip 协议也叫 Epidemic 协议 （流行病协议）。原本用于分布式数据库中节点同步数据使用，

后被广泛用于数据库复制、信息扩散、集群成员身份确认、故障探测等。

从 gossip 单词就可以看到，其中文意思是八卦、流言等意思，我们可以想象下绯闻的传播（或者流

行病的传播）；Gossip  协议的工作原理就类似于这个。Gossip 协议利用一种随机的方式将信息传播到整

个网络中，并在一定时间内使得系统内的所有节点数据一致。Gossip 协议其实是一种去中心化思路的分布式

协议，解决状态在集群中的传播和状态一致性的保证两个问题。

#### 3.3.2 Gossip原理

Gossip 协议的消息传播方式有两种：反熵传播 和 谣言传播

1. 反熵传播

反熵是指集群中的节点，每隔段时间就随机选择某个其他节点，然后通过互相交换自己的所有数据来消除两者之间的差异，实现数据的一致性。所有参与节点只有两种状态：Suspective(病原)、Infective(感染)。

过程是种子节点会把所有的数据都跟其他节点共享，以便消除节点之间数据的任何不一致，它可以保证最终、完全的一致。缺点是消息数量非常庞大，且无限制；通常只用于新加入节点的数据初始化。

![Gossip](https://gitee.com/linbingxing/image/raw/master/distributed/Gossip.png)

2. 谣言传播

   广泛地散播谣言，它是指当一个节点有了新数据后，这个节点变成活跃状态，并周期性联系其他节点向其发送新数据，直到所有的节点都存储了该新数据。所有参与节点有三种状态：Suspective(病原)、Infective(感染)、Removed(愈除)。过程是消息只包含最新 update，谣言消息在某个时间点之后会被标记为 removed，并且不再被传播。缺点是系统有一定的概率会不一致，通常用于节点间数据增量同步。

   ![Gossip-1](https://gitee.com/linbingxing/image/raw/master/distributed/Gossip-1.png)

在图中可以看到，节点A向节点B，D发送新数据，节点B收到新数据后，变成活跃状态，然后节点B向节点C，D发送新数据。

####  3.3.3  通信方式

Gossip 协议最终目的是将数据分发到网络中的每一个节点。**根据不同的具体应用场景，网络中两个节**

**点之间存在三种通信方式：推送模式、拉取模式、推拉模式**

1. Push

   节点 A 将数据 (key,value,version) 及对应的版本号推送给 节点B ， 节点B更新节点A 中比自己新的数据。

   ![Gossip-2](https://gitee.com/linbingxing/image/raw/master/distributed/Gossip-2.png)

2. Pull

   节点A 仅将数据 key, version 推送给节点 B，节点B 将本地比 节点A 新的数据（Key, value, version）推送给 节点A，节点A 更新本地。

   ![Gossip-3](https://gitee.com/linbingxing/image/raw/master/distributed/Gossip-3.png)

3. Push/Pull

   与 Pull 类似，只是多了一步，节点A 再将本地比 节点B 新的数据推送给 节点B，节点B 则更新本地。

#### **3.3.4** **优缺点**

综上所述，我们可以得出 **Gossip** **是一种去中心化的分布式协议，数据通过节点像病毒一样逐个传**

**播**。因为是指数级传播，整体传播速度非常快。

1. **优点**

- 扩展性：允许节点的任意增加和减少，新增节点的状态 最终会与其他节点一致

- 容错：任意节点的宕机和重启都不会影响 Gossip 消息的传播，具有天然的分布式系统容错特性

- 去中心化：无需中心节点，所有节点都是对等的，任意节点无需知道整个网络状况，只要网络连通，任意节点可把消息散播到全网

- 最终一致性：Gossip 协议实现信息指数级的快速传播，因此在有新信息需要传播时，消息可以快速地发送到全局节点，在有限的时间内能够做到所有节点都拥有最新的数据。

2. **缺点**

- 消息延迟:节点随机向少数几个节点发送消息，消息最终是通过多个轮次的散播而到达全网；不可避免的造成消息延迟。

- 消息冗余:节点定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤；不可避免的引起同一节点消息多次接收，增加消息处理压力。

Gossip 协议由于以上的优缺点，所以**适合于** **AP** **场景的数据一致性处理**，常见应用有：P2P 网络通信、

Redis Cluster、Consul。

###  3.4 Quorum NWP协议

#### 3.4.1 什么是Quorum  NWR协议

Quorum NWR是一种在分布式存储系统中用于控制一致性级别的一种策略。在亚马逊的云存储系统中，就应用

Quorum NWR来控制一致性。

- N：在分布式存储系统中，有多少份备份数据
- W：代表一次成功的更新操作要求至少有w份数据写入成功
- R： 代表一次成功的读数据操作要求至少有R份数据成功读取

####  **3.4.2** **原理**

Quorum NWR值的不同组合会产生不同的一致性效果，当W+R>N的时候，整个系统对于客户端来讲能保证强

一致性。

**以常见的N=3、W=2、R=2为例:**

- N=3，表示，任何一个对象都必须有三个副本
- W=2表示，对数据的修改操作只需要在3个副本中的2个上面完成就返回
- R=2表示，从三个对象中要读取到2个数据对象，才能返回

> 在分布式系统中，数据的单点是不允许存在的。即线上正常存在的备份数量N设置1的情况是
>
> 非常危险的，因为一旦这个备份发生错误，就 可能发生数据的永久性错误。假如我们把N设
>
> 置成为2，那么，只要有一个存储节点发生损坏，就会有单点的存在。所以N必须大于2。N越
>
> 高，系统的维护和整体 成本就越高。工业界通常把N设置为3。 

1. 当W是2、R是2的时候，W+R>N，这种情况对于客户端就是强一致性的。

   ![NWR-1](https://gitee.com/linbingxing/image/raw/master/distributed/NWR-1.png)

   在上图中，如果R+W>N,则读取操作和写入操作成功的数据一定会有交集（如图中的

   Node2），这样就可以保证一定能够读取到最新版本的更新数据，数据的强一致性得到了保证。在

   满足数据一致性协议的前提下，R或者W设置的越大，则系统延迟越大，因为这取决于最慢的那份

   备份数据的响应时间。

2. 当R+W<=N，无法保证数据的强一致性

   ![NWR-2](https://gitee.com/linbingxing/image/raw/master/distributed/NWR-2.png)

   因为成功写和成功读集合可能不存在交集，这样读操作无法读取到最新的更新数值，也就无法保证

   数据的强一致性。

### 3.5  Lease 机制

master给各个slave分配不同的数据，每个节点的数据都具有有效时间比如1小时，在lease时间内，客户端可以直接向slave请求数据，如果超过时间客户端就去master请求数据。一般而言，slave可以定时主动向master要求续租并更新数据，master在数据发生变化时也可以主动通知slave，不同方式的选择也在于可用性与一致性之间进行权衡。

租约机制也可以解决主备之间网络不通导致的双主脑裂问题，亦即：主备之间本来心跳连线的，但是突然之间网络不通或者暂停又恢复了或者太繁忙无法回复，这时备机开始接管服务，但是主机依然存活能对外服务，这是就发生争夺与分区，但是引入lease的话，老主机颁发给具体server的lease必然较旧，请求就失效了，老主机自动退出对外服务，备机完全接管服务。

















https://mp.weixin.qq.com/s/QynAYHUq9OZ09qiWEPPKBA



